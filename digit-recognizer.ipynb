{"cells":[{"cell_type":"code","execution_count":15,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-02-15T19:41:02.686866Z","iopub.status.busy":"2024-02-15T19:41:02.686367Z","iopub.status.idle":"2024-02-15T19:41:02.698095Z","shell.execute_reply":"2024-02-15T19:41:02.696676Z","shell.execute_reply.started":"2024-02-15T19:41:02.686823Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["/Users/shirabroner/Documents/DataScience/kaggle-data/digit-recognizer/test.csv\n","/Users/shirabroner/Documents/DataScience/kaggle-data/digit-recognizer/train.csv\n","/Users/shirabroner/Documents/DataScience/kaggle-data/digit-recognizer/sample_submission.csv\n"]}],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","import torch\n","from matplotlib import pyplot as plt\n","from sklearn.model_selection import train_test_split\n","from torchvision.transforms import v2\n","import random\n","\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","source_dir = '/kaggle/input' if os.path.exists('/kaggle') else os.environ.get('KAGGLE_DATA') + '/digit-recognizer'\n","for dirname, _, filenames in os.walk(source_dir):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-02-15T18:17:16.337571Z","iopub.status.busy":"2024-02-15T18:17:16.337155Z","iopub.status.idle":"2024-02-15T18:17:20.147168Z","shell.execute_reply":"2024-02-15T18:17:20.145591Z","shell.execute_reply.started":"2024-02-15T18:17:16.337542Z"},"trusted":true},"outputs":[],"source":["train_df = pd.read_csv(source_dir + '/train.csv')"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-02-15T18:18:37.708557Z","iopub.status.busy":"2024-02-15T18:18:37.708149Z","iopub.status.idle":"2024-02-15T18:18:38.018909Z","shell.execute_reply":"2024-02-15T18:18:38.017699Z","shell.execute_reply.started":"2024-02-15T18:18:37.708526Z"},"trusted":true},"outputs":[{"data":{"text/plain":["<Axes: xlabel='label'>"]},"execution_count":17,"metadata":{},"output_type":"execute_result"},{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["train_df['label'].value_counts().plot.bar()"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-02-15T18:32:15.718215Z","iopub.status.busy":"2024-02-15T18:32:15.717783Z","iopub.status.idle":"2024-02-15T18:32:15.868561Z","shell.execute_reply":"2024-02-15T18:32:15.867058Z","shell.execute_reply.started":"2024-02-15T18:32:15.718184Z"},"trusted":true},"outputs":[{"data":{"text/plain":["torch.Size([28, 28])"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["torch.from_numpy(train_df.filter(train_df.columns[1:]).iloc[0].values).reshape(28, 28).shape"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-02-15T18:44:42.302048Z","iopub.status.busy":"2024-02-15T18:44:42.301456Z","iopub.status.idle":"2024-02-15T18:44:42.323277Z","shell.execute_reply":"2024-02-15T18:44:42.321741Z","shell.execute_reply.started":"2024-02-15T18:44:42.302005Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>label</th>\n","      <th>pixel0</th>\n","      <th>pixel1</th>\n","      <th>pixel2</th>\n","      <th>pixel3</th>\n","      <th>pixel4</th>\n","      <th>pixel5</th>\n","      <th>pixel6</th>\n","      <th>pixel7</th>\n","      <th>pixel8</th>\n","      <th>...</th>\n","      <th>pixel774</th>\n","      <th>pixel775</th>\n","      <th>pixel776</th>\n","      <th>pixel777</th>\n","      <th>pixel778</th>\n","      <th>pixel779</th>\n","      <th>pixel780</th>\n","      <th>pixel781</th>\n","      <th>pixel782</th>\n","      <th>pixel783</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>3 rows Ã— 785 columns</p>\n","</div>"],"text/plain":["   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n","0      1       0       0       0       0       0       0       0       0   \n","1      0       0       0       0       0       0       0       0       0   \n","2      1       0       0       0       0       0       0       0       0   \n","\n","   pixel8  ...  pixel774  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n","0       0  ...         0         0         0         0         0         0   \n","1       0  ...         0         0         0         0         0         0   \n","2       0  ...         0         0         0         0         0         0   \n","\n","   pixel780  pixel781  pixel782  pixel783  \n","0         0         0         0         0  \n","1         0         0         0         0  \n","2         0         0         0         0  \n","\n","[3 rows x 785 columns]"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["train_df.head(3)"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-02-15T20:32:45.947461Z","iopub.status.busy":"2024-02-15T20:32:45.946987Z","iopub.status.idle":"2024-02-15T20:32:47.363512Z","shell.execute_reply":"2024-02-15T20:32:47.362226Z","shell.execute_reply.started":"2024-02-15T20:32:45.947426Z"},"trusted":true},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 1000x400 with 10 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["images = train_df.sample(frac=1).reset_index(drop=True).groupby(by='label').first().reset_index(drop=False).filter(train_df.columns[1:]).values.reshape(10, 28, 28)\n","fig, axes = plt.subplots(2, 5, figsize=(10, 4))\n","\n","axes = axes.flatten()\n","\n","for i, ax in enumerate(axes):\n","    ax.imshow(images[i], cmap='gray')\n","    ax.axis('off')\n","\n","plt.tight_layout() \n","plt.show()"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-02-15T19:33:56.186417Z","iopub.status.busy":"2024-02-15T19:33:56.185973Z","iopub.status.idle":"2024-02-15T19:33:56.732549Z","shell.execute_reply":"2024-02-15T19:33:56.731494Z","shell.execute_reply.started":"2024-02-15T19:33:56.186384Z"},"trusted":true},"outputs":[],"source":["label = torch.tensor(train_df['label'])\n","train = torch.tensor(np.array(train_df.drop(['label'], axis = 1)), dtype=torch.float32).reshape(len(train_df), 1, 28, 28)"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-02-15T20:31:56.858313Z","iopub.status.busy":"2024-02-15T20:31:56.857460Z","iopub.status.idle":"2024-02-15T20:31:57.292565Z","shell.execute_reply":"2024-02-15T20:31:57.291229Z","shell.execute_reply.started":"2024-02-15T20:31:56.858270Z"},"trusted":true},"outputs":[{"data":{"text/plain":["(33600, 8400, 33600, 8400)"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["X_train, X_val, y_train, y_val = train_test_split(train, label ,test_size = 0.20,random_state = 42, stratify=label)\n","len(X_train), len(X_val), len(y_train), len(y_val)"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2024-02-15T20:32:38.426081Z","iopub.status.busy":"2024-02-15T20:32:38.425636Z","iopub.status.idle":"2024-02-15T20:32:38.563248Z","shell.execute_reply":"2024-02-15T20:32:38.561930Z","shell.execute_reply.started":"2024-02-15T20:32:38.426050Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["0\n"]},{"data":{"image/png":"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","text/plain":["<Figure size 1000x400 with 2 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["index = random.randint(0, len(X_train))\n","image =  X_train[index]\n","print(y_train[index].item())\n","\n","# Define transformations\n","transform = v2.Compose([# Convert image to PyTorch tensor,\n","    v2.RandomResizedCrop(scale=(0.7, 1), size=(28, 28), antialias=True),\n","    v2.RandomHorizontalFlip(p=0.3),\n","    v2.Normalize((0,), (1,)),\n","    v2.RandomApply([v2.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.2, hue=0.1)], p=0.6),\n","    v2.RandomAffine(30, scale=(0.95, 1))\n","])\n","\n","# Assuming 'image' is your 28x28 image\n","processed_image = transform(image)\n","images = [image, processed_image]\n","fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n","\n","axes = axes.flatten()\n","\n","for i, ax in enumerate(axes):\n","    ax.imshow(images[i].numpy().reshape(28, 28), cmap='gray')\n","    ax.axis('off')\n","\n","plt.tight_layout() \n","plt.show()"]},{"cell_type":"code","execution_count":45,"metadata":{"execution":{"iopub.execute_input":"2024-02-15T20:48:43.747801Z","iopub.status.busy":"2024-02-15T20:48:43.747277Z","iopub.status.idle":"2024-02-15T20:48:44.213698Z","shell.execute_reply":"2024-02-15T20:48:44.212555Z","shell.execute_reply.started":"2024-02-15T20:48:43.747765Z"},"trusted":true},"outputs":[{"data":{"text/plain":["ResNetForImageClassification(\n","  (resnet): ResNetModel(\n","    (embedder): ResNetEmbeddings(\n","      (embedder): ResNetConvLayer(\n","        (convolution): Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","        (normalization): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (activation): ReLU()\n","      )\n","      (pooler): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","    )\n","    (encoder): ResNetEncoder(\n","      (stages): ModuleList(\n","        (0): ResNetStage(\n","          (layers): Sequential(\n","            (0): ResNetBottleNeckLayer(\n","              (shortcut): ResNetShortCut(\n","                (convolution): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","              )\n","              (layer): Sequential(\n","                (0): ResNetConvLayer(\n","                  (convolution): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (normalization): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (activation): ReLU()\n","                )\n","                (1): ResNetConvLayer(\n","                  (convolution): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","                  (normalization): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (activation): ReLU()\n","                )\n","                (2): ResNetConvLayer(\n","                  (convolution): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (activation): Identity()\n","                )\n","              )\n","              (activation): ReLU()\n","            )\n","            (1): ResNetBottleNeckLayer(\n","              (shortcut): Identity()\n","              (layer): Sequential(\n","                (0): ResNetConvLayer(\n","                  (convolution): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (normalization): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (activation): ReLU()\n","                )\n","                (1): ResNetConvLayer(\n","                  (convolution): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","                  (normalization): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (activation): ReLU()\n","                )\n","                (2): ResNetConvLayer(\n","                  (convolution): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (activation): Identity()\n","                )\n","              )\n","              (activation): ReLU()\n","            )\n","            (2): ResNetBottleNeckLayer(\n","              (shortcut): Identity()\n","              (layer): Sequential(\n","                (0): ResNetConvLayer(\n","                  (convolution): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (normalization): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (activation): ReLU()\n","                )\n","                (1): ResNetConvLayer(\n","                  (convolution): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","                  (normalization): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (activation): ReLU()\n","                )\n","                (2): ResNetConvLayer(\n","                  (convolution): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (activation): Identity()\n","                )\n","              )\n","              (activation): ReLU()\n","            )\n","          )\n","        )\n","        (1): ResNetStage(\n","          (layers): Sequential(\n","            (0): ResNetBottleNeckLayer(\n","              (shortcut): ResNetShortCut(\n","                (convolution): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","                (normalization): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","              )\n","              (layer): Sequential(\n","                (0): ResNetConvLayer(\n","                  (convolution): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (normalization): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (activation): ReLU()\n","                )\n","                (1): ResNetConvLayer(\n","                  (convolution): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","                  (normalization): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (activation): ReLU()\n","                )\n","                (2): ResNetConvLayer(\n","                  (convolution): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (normalization): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (activation): Identity()\n","                )\n","              )\n","              (activation): ReLU()\n","            )\n","            (1): ResNetBottleNeckLayer(\n","              (shortcut): Identity()\n","              (layer): Sequential(\n","                (0): ResNetConvLayer(\n","                  (convolution): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (normalization): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (activation): ReLU()\n","                )\n","                (1): ResNetConvLayer(\n","                  (convolution): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","                  (normalization): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (activation): ReLU()\n","                )\n","                (2): ResNetConvLayer(\n","                  (convolution): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (normalization): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (activation): Identity()\n","                )\n","              )\n","              (activation): ReLU()\n","            )\n","            (2): ResNetBottleNeckLayer(\n","              (shortcut): Identity()\n","              (layer): Sequential(\n","                (0): ResNetConvLayer(\n","                  (convolution): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (normalization): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (activation): ReLU()\n","                )\n","                (1): ResNetConvLayer(\n","                  (convolution): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","                  (normalization): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (activation): ReLU()\n","                )\n","                (2): ResNetConvLayer(\n","                  (convolution): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (normalization): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (activation): Identity()\n","                )\n","              )\n","              (activation): ReLU()\n","            )\n","            (3): ResNetBottleNeckLayer(\n","              (shortcut): Identity()\n","              (layer): Sequential(\n","                (0): ResNetConvLayer(\n","                  (convolution): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (normalization): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (activation): ReLU()\n","                )\n","                (1): ResNetConvLayer(\n","                  (convolution): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","                  (normalization): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (activation): ReLU()\n","                )\n","                (2): ResNetConvLayer(\n","                  (convolution): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (normalization): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (activation): Identity()\n","                )\n","              )\n","              (activation): ReLU()\n","            )\n","          )\n","        )\n","        (2): ResNetStage(\n","          (layers): Sequential(\n","            (0): ResNetBottleNeckLayer(\n","              (shortcut): ResNetShortCut(\n","                (convolution): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","                (normalization): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","              )\n","              (layer): Sequential(\n","                (0): ResNetConvLayer(\n","                  (convolution): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (activation): ReLU()\n","                )\n","                (1): ResNetConvLayer(\n","                  (convolution): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","                  (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (activation): ReLU()\n","                )\n","                (2): ResNetConvLayer(\n","                  (convolution): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (normalization): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (activation): Identity()\n","                )\n","              )\n","              (activation): ReLU()\n","            )\n","            (1): ResNetBottleNeckLayer(\n","              (shortcut): Identity()\n","              (layer): Sequential(\n","                (0): ResNetConvLayer(\n","                  (convolution): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (activation): ReLU()\n","                )\n","                (1): ResNetConvLayer(\n","                  (convolution): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","                  (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (activation): ReLU()\n","                )\n","                (2): ResNetConvLayer(\n","                  (convolution): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (normalization): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (activation): Identity()\n","                )\n","              )\n","              (activation): ReLU()\n","            )\n","            (2): ResNetBottleNeckLayer(\n","              (shortcut): Identity()\n","              (layer): Sequential(\n","                (0): ResNetConvLayer(\n","                  (convolution): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (activation): ReLU()\n","                )\n","                (1): ResNetConvLayer(\n","                  (convolution): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","                  (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (activation): ReLU()\n","                )\n","                (2): ResNetConvLayer(\n","                  (convolution): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (normalization): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (activation): Identity()\n","                )\n","              )\n","              (activation): ReLU()\n","            )\n","            (3): ResNetBottleNeckLayer(\n","              (shortcut): Identity()\n","              (layer): Sequential(\n","                (0): ResNetConvLayer(\n","                  (convolution): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (activation): ReLU()\n","                )\n","                (1): ResNetConvLayer(\n","                  (convolution): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","                  (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (activation): ReLU()\n","                )\n","                (2): ResNetConvLayer(\n","                  (convolution): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (normalization): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (activation): Identity()\n","                )\n","              )\n","              (activation): ReLU()\n","            )\n","            (4): ResNetBottleNeckLayer(\n","              (shortcut): Identity()\n","              (layer): Sequential(\n","                (0): ResNetConvLayer(\n","                  (convolution): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (activation): ReLU()\n","                )\n","                (1): ResNetConvLayer(\n","                  (convolution): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","                  (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (activation): ReLU()\n","                )\n","                (2): ResNetConvLayer(\n","                  (convolution): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (normalization): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (activation): Identity()\n","                )\n","              )\n","              (activation): ReLU()\n","            )\n","            (5): ResNetBottleNeckLayer(\n","              (shortcut): Identity()\n","              (layer): Sequential(\n","                (0): ResNetConvLayer(\n","                  (convolution): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (activation): ReLU()\n","                )\n","                (1): ResNetConvLayer(\n","                  (convolution): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","                  (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (activation): ReLU()\n","                )\n","                (2): ResNetConvLayer(\n","                  (convolution): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (normalization): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (activation): Identity()\n","                )\n","              )\n","              (activation): ReLU()\n","            )\n","          )\n","        )\n","        (3): ResNetStage(\n","          (layers): Sequential(\n","            (0): ResNetBottleNeckLayer(\n","              (shortcut): ResNetShortCut(\n","                (convolution): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","                (normalization): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","              )\n","              (layer): Sequential(\n","                (0): ResNetConvLayer(\n","                  (convolution): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (normalization): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (activation): ReLU()\n","                )\n","                (1): ResNetConvLayer(\n","                  (convolution): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","                  (normalization): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (activation): ReLU()\n","                )\n","                (2): ResNetConvLayer(\n","                  (convolution): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (normalization): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (activation): Identity()\n","                )\n","              )\n","              (activation): ReLU()\n","            )\n","            (1): ResNetBottleNeckLayer(\n","              (shortcut): Identity()\n","              (layer): Sequential(\n","                (0): ResNetConvLayer(\n","                  (convolution): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (normalization): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (activation): ReLU()\n","                )\n","                (1): ResNetConvLayer(\n","                  (convolution): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","                  (normalization): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (activation): ReLU()\n","                )\n","                (2): ResNetConvLayer(\n","                  (convolution): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (normalization): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (activation): Identity()\n","                )\n","              )\n","              (activation): ReLU()\n","            )\n","            (2): ResNetBottleNeckLayer(\n","              (shortcut): Identity()\n","              (layer): Sequential(\n","                (0): ResNetConvLayer(\n","                  (convolution): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (normalization): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (activation): ReLU()\n","                )\n","                (1): ResNetConvLayer(\n","                  (convolution): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","                  (normalization): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (activation): ReLU()\n","                )\n","                (2): ResNetConvLayer(\n","                  (convolution): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (normalization): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (activation): Identity()\n","                )\n","              )\n","              (activation): ReLU()\n","            )\n","          )\n","        )\n","      )\n","    )\n","    (pooler): AdaptiveAvgPool2d(output_size=(1, 1))\n","  )\n","  (classifier): Sequential(\n","    (0): Flatten(start_dim=1, end_dim=-1)\n","    (1): Linear(in_features=2048, out_features=10, bias=True)\n","  )\n",")"]},"execution_count":45,"metadata":{},"output_type":"execute_result"}],"source":["from transformers import AutoModelForSequenceClassification\n","from transformers import ResNetConfig, ResNetModel\n","from transformers import AutoImageProcessor, ResNetForImageClassification\n","\n","#model = ResNetForImageClassification.from_pretrained(\"microsoft/resnet-50\", num_labels=10, ignore_mismatched_sizes=True)\n","# Initializing a ResNet resnet-50 style configuration\n","configuration = ResNetConfig(num_channels=1, num_labels=10)\n","\n","# Initializing a model (with random weights) from the resnet-50 style configuration\n","model = ResNetForImageClassification(configuration)\n","\n","model\n","#model = AutoModelForSequenceClassification.from_pretrained(\"microsoft/resnet-50\", num_labels=10)"]},{"cell_type":"code","execution_count":46,"metadata":{"execution":{"iopub.execute_input":"2024-02-15T20:29:53.545276Z","iopub.status.busy":"2024-02-15T20:29:53.544845Z","iopub.status.idle":"2024-02-15T20:29:53.562688Z","shell.execute_reply":"2024-02-15T20:29:53.561404Z","shell.execute_reply.started":"2024-02-15T20:29:53.545245Z"},"trusted":true},"outputs":[],"source":["import torch\n","from torch.utils.data import Dataset, DataLoader\n","\n","# Define custom dataset class\n","class DigitImageDataset(Dataset):\n","    def __init__(self, data, labels):\n","        self.data = data\n","        self.labels = labels\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        sample = (self.data[idx], self.labels[idx])\n","        return sample\n","\n","# Assuming you have your data and labels in variables 'data' and 'labels'\n","# Create an instance of CustomDataset\n","dataset = DigitImageDataset(X_train, y_train)\n","\n","# Define batch size\n","batch_size = 32\n","\n","# Create DataLoader\n","dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n","for x1, x2 in dataloader:\n","    break"]},{"cell_type":"code","execution_count":48,"metadata":{"execution":{"iopub.execute_input":"2024-02-15T20:33:19.236764Z","iopub.status.busy":"2024-02-15T20:33:19.236307Z","iopub.status.idle":"2024-02-15T20:33:19.243473Z","shell.execute_reply":"2024-02-15T20:33:19.242079Z","shell.execute_reply.started":"2024-02-15T20:33:19.236732Z"},"trusted":true},"outputs":[],"source":["train_dataloader = DataLoader(DigitImageDataset(X_train, y_train), batch_size=64)\n","val_dataloader = DataLoader(DigitImageDataset(X_val, y_val), batch_size=32)"]},{"cell_type":"code","execution_count":49,"metadata":{"execution":{"iopub.execute_input":"2024-02-15T20:45:52.424220Z","iopub.status.busy":"2024-02-15T20:45:52.423743Z","iopub.status.idle":"2024-02-15T20:45:52.433409Z","shell.execute_reply":"2024-02-15T20:45:52.431980Z","shell.execute_reply.started":"2024-02-15T20:45:52.424185Z"},"trusted":true},"outputs":[],"source":["from torch.optim import Adam\n","\n","optimizer = Adam(model.parameters(), lr=5e-5)\n","loss_fn = torch.nn.CrossEntropyLoss()\n"]},{"cell_type":"code","execution_count":50,"metadata":{"execution":{"iopub.execute_input":"2024-02-15T20:33:24.061229Z","iopub.status.busy":"2024-02-15T20:33:24.060811Z","iopub.status.idle":"2024-02-15T20:33:24.068265Z","shell.execute_reply":"2024-02-15T20:33:24.066947Z","shell.execute_reply.started":"2024-02-15T20:33:24.061199Z"},"trusted":true},"outputs":[],"source":["from transformers import get_scheduler\n","\n","num_epochs = 3\n","num_training_steps = num_epochs * len(train_dataloader)\n","lr_scheduler = get_scheduler(\n","    name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",")"]},{"cell_type":"code","execution_count":51,"metadata":{"execution":{"iopub.execute_input":"2024-02-15T20:49:54.403056Z","iopub.status.busy":"2024-02-15T20:49:54.402614Z","iopub.status.idle":"2024-02-15T20:49:54.412946Z","shell.execute_reply":"2024-02-15T20:49:54.411291Z","shell.execute_reply.started":"2024-02-15T20:49:54.403024Z"},"trusted":true},"outputs":[],"source":["def train_one_epoch(epoch_index, tb_writer):\n","    running_loss = 0.\n","    last_loss = 0.\n","\n","    # Here, we use enumerate(training_loader) instead of\n","    # iter(training_loader) so that we can track the batch\n","    # index and do some intra-epoch reporting\n","    for i, data in enumerate(train_dataloader):\n","        # Every data instance is an input + label pair\n","        inputs, labels = data\n","\n","        # Zero your gradients for every batch!\n","        optimizer.zero_grad()\n","\n","        # Make predictions for this batch\n","        outputs = model(inputs).logits\n","\n","        # Compute the loss and its gradients\n","        loss = loss_fn(outputs, labels)\n","        loss.backward()\n","\n","        # Adjust learning weights\n","        optimizer.step()\n","\n","        # Gather data and report\n","        running_loss += loss.item()\n","        if i % 1000 == 999:\n","            last_loss = running_loss / 1000 # loss per batch\n","            print('  batch {} loss: {}'.format(i + 1, last_loss))\n","            tb_x = epoch_index * len(training_loader) + i + 1\n","            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n","            running_loss = 0.\n","\n","    return last_loss"]},{"cell_type":"code","execution_count":52,"metadata":{"execution":{"iopub.execute_input":"2024-02-15T20:49:55.929282Z","iopub.status.busy":"2024-02-15T20:49:55.928836Z","iopub.status.idle":"2024-02-15T20:49:56.431339Z","shell.execute_reply":"2024-02-15T20:49:56.429576Z","shell.execute_reply.started":"2024-02-15T20:49:55.929247Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["EPOCH 1:\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[52], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Make sure gradient tracking is on, and do a pass over the data\u001b[39;00m\n\u001b[1;32m     16\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 17\u001b[0m avg_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch_number\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m running_vloss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Set the model to evaluation mode, disabling dropout and using population\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# statistics for batch normalization.\u001b[39;00m\n","Cell \u001b[0;32mIn[51], line 16\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(epoch_index, tb_writer)\u001b[0m\n\u001b[1;32m     13\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Make predictions for this batch\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlogits\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Compute the loss and its gradients\u001b[39;00m\n\u001b[1;32m     19\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(outputs, labels)\n","File \u001b[0;32m~/Documents/DataScience/venv/Default/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/Documents/DataScience/venv/Default/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m~/Documents/DataScience/venv/Default/lib/python3.9/site-packages/transformers/models/resnet/modeling_resnet.py:390\u001b[0m, in \u001b[0;36mResNetForImageClassification.forward\u001b[0;34m(self, pixel_values, labels, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;124;03m    Labels for computing the image classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    388\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m--> 390\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    392\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mpooler_output \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;28;01melse\u001b[39;00m outputs[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    394\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(pooled_output)\n","File \u001b[0;32m~/Documents/DataScience/venv/Default/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/Documents/DataScience/venv/Default/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m~/Documents/DataScience/venv/Default/lib/python3.9/site-packages/transformers/models/resnet/modeling_resnet.py:331\u001b[0m, in \u001b[0;36mResNetModel.forward\u001b[0;34m(self, pixel_values, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    327\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m    329\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedder(pixel_values)\n\u001b[0;32m--> 331\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    335\u001b[0m last_hidden_state \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    337\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(last_hidden_state)\n","File \u001b[0;32m~/Documents/DataScience/venv/Default/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/Documents/DataScience/venv/Default/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m~/Documents/DataScience/venv/Default/lib/python3.9/site-packages/transformers/models/resnet/modeling_resnet.py:237\u001b[0m, in \u001b[0;36mResNetEncoder.forward\u001b[0;34m(self, hidden_state, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[1;32m    235\u001b[0m         hidden_states \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m+\u001b[39m (hidden_state,)\n\u001b[0;32m--> 237\u001b[0m     hidden_state \u001b[38;5;241m=\u001b[39m \u001b[43mstage_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[1;32m    240\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m+\u001b[39m (hidden_state,)\n","File \u001b[0;32m~/Documents/DataScience/venv/Default/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/Documents/DataScience/venv/Default/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m~/Documents/DataScience/venv/Default/lib/python3.9/site-packages/transformers/models/resnet/modeling_resnet.py:206\u001b[0m, in \u001b[0;36mResNetStage.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    204\u001b[0m hidden_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m--> 206\u001b[0m     hidden_state \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m hidden_state\n","File \u001b[0;32m~/Documents/DataScience/venv/Default/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/Documents/DataScience/venv/Default/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m~/Documents/DataScience/venv/Default/lib/python3.9/site-packages/transformers/models/resnet/modeling_resnet.py:173\u001b[0m, in \u001b[0;36mResNetBottleNeckLayer.forward\u001b[0;34m(self, hidden_state)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_state):\n\u001b[1;32m    172\u001b[0m     residual \u001b[38;5;241m=\u001b[39m hidden_state\n\u001b[0;32m--> 173\u001b[0m     hidden_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    174\u001b[0m     residual \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshortcut(residual)\n\u001b[1;32m    175\u001b[0m     hidden_state \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m residual\n","File \u001b[0;32m~/Documents/DataScience/venv/Default/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/Documents/DataScience/venv/Default/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m~/Documents/DataScience/venv/Default/lib/python3.9/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n","File \u001b[0;32m~/Documents/DataScience/venv/Default/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/Documents/DataScience/venv/Default/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m~/Documents/DataScience/venv/Default/lib/python3.9/site-packages/transformers/models/resnet/modeling_resnet.py:74\u001b[0m, in \u001b[0;36mResNetConvLayer.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 74\u001b[0m     hidden_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvolution\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m     hidden_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalization(hidden_state)\n\u001b[1;32m     76\u001b[0m     hidden_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation(hidden_state)\n","File \u001b[0;32m~/Documents/DataScience/venv/Default/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/Documents/DataScience/venv/Default/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m~/Documents/DataScience/venv/Default/lib/python3.9/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/Documents/DataScience/venv/Default/lib/python3.9/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["from  datetime import datetime\n","from torch.utils.tensorboard import SummaryWriter\n","# Initializing in a separate cell so we can easily add more epochs to the same run\n","timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n","writer = SummaryWriter('runs/digits_trainer_{}'.format(timestamp))\n","epoch_number = 0\n","\n","EPOCHS = 5\n","\n","best_vloss = 1_000_000.\n","\n","for epoch in range(EPOCHS):\n","    print('EPOCH {}:'.format(epoch_number + 1))\n","\n","    # Make sure gradient tracking is on, and do a pass over the data\n","    model.train(True)\n","    avg_loss = train_one_epoch(epoch_number, writer)\n","\n","\n","    running_vloss = 0.0\n","    # Set the model to evaluation mode, disabling dropout and using population\n","    # statistics for batch normalization.\n","    model.eval()\n","\n","    # Disable gradient computation and reduce memory consumption.\n","    with torch.no_grad():\n","        for i, vdata in enumerate(val_dataloader):\n","            vinputs, vlabels = vdata\n","            voutputs = model(vinputs)\n","            vloss = loss_fn(voutputs, vlabels)\n","            running_vloss += vloss\n","\n","    avg_vloss = running_vloss / (i + 1)\n","    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n","\n","    # Log the running loss averaged per batch\n","    # for both training and validation\n","    writer.add_scalars('Training vs. Validation Loss',\n","                    { 'Training' : avg_loss, 'Validation' : avg_vloss },\n","                    epoch_number + 1)\n","    writer.flush()\n","\n","    # Track best performance, and save the model's state\n","    if avg_vloss < best_vloss:\n","        best_vloss = avg_vloss\n","        model_path = 'model_{}_{}'.format(timestamp, epoch_number)\n","        torch.save(model.state_dict(), model_path)\n","\n","    epoch_number += 1"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
